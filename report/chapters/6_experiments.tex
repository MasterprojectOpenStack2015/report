\section[Running Dependability Experiments \texorpdfstring{{\textbf{\tiny \enspace (JE, SK)}}}{}]{Running Dependability Experiments}
\sectionmark{Running Dependability Experiments}
\label{experiments}
In this chapter, we will describe our OpenStack dependability experiments and their results. The implementations of these experiments all follow the same structure, so that it is easy to add further experiments if needed. In our experiments, we focused on covering the failure of various OpenStack components or nodes. We will give insights on how these failures affect the OpenStack environment and how the system deals with each fault. This serves partly to show weak points of the setup and partly to document details about its behavior.\\

Experiments consist of multiple stages: The \emph{setup} stage creates all elements necessary to run the experiment. The \emph{break} stage then breaks things. An optional \emph{heal} stage tries something simple to undo the damage (e.g. reboot a shutdown node). After each of this stages, a \emph{check} step is executed, which observes the state of the system and reports its findings to the user. Generally, after the setup stage all checks should be successful. Where user observations are useful (e.g. by looking not just at API results, but seeing how Horizon represents situations), the user is prompted to do so.\\

Using the snapshotting mechanism, the user can always completely restore the system, even if it didn't survive an experiment. This is not done by default because a) it takes some time and is not always necessary and b) to allow the user to inspect the system state after the experiment has concluded, e.g. to find or work out steps to fix remaining issues.

\subsection{How to Run the Experiments}

Experiments can be run by the shell script \texttt{run\_experiment.sh}. It lists the experiments and allows to run a specific one. It also allows to choose between different verbosity levels. The default verbosity \texttt{-v} shows a structured high-level overview with enough output to follow the experiment. \texttt{-vv} shows additional details and asks for confirmation between steps to give the user time to inspect system state manually if desired. The highest level \texttt{-vvv} shows all the output generated by commands in the experiment. This is useful to understand the how the experiment works and to debug potential issues during experiment development.\\

Internally, each experiment has a script \texttt{run.sh} that launches the individual steps, which live in specially formatted scripts of their own.

\subsection{Experiment Results}
This section describes our four example experiments and discusses their results. 

\subsubsection{Experiment 1: Control Node crash}
In this experiment, a crash of the control node in the system is simulated. \\

\textbf{Technical Background}\\
The controller node stores global information of the OpenStack cluster and runs the sub-services depending on it, which then give services on other nodes the specific information they need to e.g. run a specific instance or to build network connectivity. In our setup, it also provides the dashboard Horizon and runs the authentication service. A failure of this node obviously is going to have a large impact, but some things that already are set up on other nodes continue to operate.\\

\textbf{Experiment}\\
The experiment script creates an instance and then uses ping to verify availability of both the compute node and the started instance. It also tries to access OpenStack APIs. To simulate the fault, it either issues a shutdown command, simulating the unavailability of the node to the controller or more severely, just turns off its infrastructure virtual machine to simulate a full crash.\\

\textbf{Results and Conclusion}\\
While the control node is turned off, outside connectivity to the already created instance remains, since it runs on the compute node and its network connection to the outside (managed by Neutron, via the network node) is unaffected. On the other hand, all attempts to use OpenStack APIs fail. Most user-facing APIs are accessed via the controller and thus completely unavailable. Others report errors, since every action has to be authorized using the Keystone service, which only runs on the compute node in our setup.\\

After the controller node is up again, OpenStack takes a few minutes to reestablish all service connections and in most cases is fully operational again. It is possible that the compute instances fail to reconnect to RabbitMQ and their services have to be restarted manually\footnote{\url{http://docs.openstack.org/openstack-ops/content/maintenance.html\#cloud_controller_storage}}. \\

This shows that already running instances on OpenStack are generally not impacted by temporary failure or maintenance of quite a few central OpenStack components, but during their inavailability no changes can be made and recovery might need manual intervention by an operator. \\

If the controller node was shut down hard, obviously many more failure scenarios related to the underlying operating system or services are possible, e. g. missing information in databases or damaged file systems.\\

\subsubsection{Experiment 2: Memchached Service Loss of Data}
This experiment shows the effects of Keystone losing authentication tokens due to the design of its storage mechanism.\\

\textbf{Technical Background}\\
A common way to authenticate for operations against the OpenStack API is to use tokens. A more complex and privileged authentication (e.g. a passsword check) is done once to obtain a security token. These tokens have a limited lifetime and can be limited in scope, so a user can generate a token for a specific task and pass it to a service, which then can use it to access other services in the users name. Tokens also are internal to OpenStack, whereas other authentication might require accessing an external authentication provider (e.g. an LDAP server).\\

The Keystone service stores these tokens in memcached\footnote{\url{http://memcached.org/}}, which is, as the name alludes to, an in-memory caching service. Designed as just a fast caching layer, it neither has persistence to disk nor does it guarantee to keep all data it stores. If it deems necessary it can evict any information at any time (i.e. because it is under memory pressure).\footnote{\url{https://code.google.com/p/memcached/wiki/NewUserInternals\#How_the_LRU_Decides_What_to_Evict}}\\

\textbf{Experiment}\\
The admin credentials are used to create a token. To verify the tokens validity, it is used to authenticate an operation against the Keystone API. To cause memcached to evict it from the cache, the command for memcached to delete all its data is issued. \\

\textbf{Results and Conclusion}\\
Subsequent attempts to use the token fail, since it has been deleted. This shows the consequences of OpenStack using an unreliable data store to save a central element of its authentication system, instead of using memcached as designed only as a cache to improve lookup speeds. \\

If a user is  logged into the Horizon dashbord while the experiment is running, it also sometimes allows to observe failures: The dashboard site is still accessible (because the session on the web server still exists), but no information from OpenStack is shown because attempts to retrieve it fail due to the invalid token. The user has to log out and back in again to create a new token. 

The implementation of this experiment is described in Appendix~\ref{a:memcache}

\subsubsection{Experiment 3: Compute Nodes Unavailable}
This experiment documents the behavior if connectivity to compute nodes is lost.\\

\textbf{Technical Background}\\
Instances are distributed among the compute nodes by the Nova scheduler, which runs on the controller node. Inspecting the state of VMs, e. g. directly via the Nova API or via the Horizon Dashboard is also done via information stored by Nova on the controller node, where it collects information from all compute hosts.\\
If the controller looses connectivity to compute nodes, it can't accurately report the state of individual instances, as seen in this experiment. \\

\textbf{Experiment}\\
This experiment first creates an instance to observe throughout the different stages. Then the first level of fault is introduced: All compute nodes are removed from the management network. Then an attempt to start a second instance is made. After this, a more severe fault is created by shutting down all compute nodes. Then a "fix" is attempted by restarting the VMs.\\

\textbf{Results and Conclusion}\\
After the first fault,  Nova on the controller has no connectivity to the compute nodes and is therefore unable to actually start the second instance. The first instance is still reported as being active, which in this case is correct: it is still running and can be reached from the outside network. Since it already has lost all connectivity to the compute nodes, things look exactly the same after the compute nodes are actually shut down, but in this case the state information is wrong: the VMs obviously are not active, but shut down together with the host they were running on. After the reboot of the compute nodes, they reconnect to the controller and the state of all VMs is reported correctly again.\\

This shows that when Nova looses connectivity to a compute node it can't report accurate information, which is one of the reasons why Nova doesn't implement functionality to automatically restart such VMs.  Short interruption of Nova services, e.g. due to software updates, do not necessarily disrupt instance operations. Such decisions are left to other systems that can collect more information (e.g. by running active tests against VMs) and can be configured for specific application needs, like OpenStack Heat. \\

We find it interesting that the developers choose to report such instances as active and did not introduce another state to represent this special situation.

\subsubsection{Experiment 4: Object Storage Crash}
In this experiment, the crash of object storage (Swift) nodes is simulated. The experiment is setup by uploading a 100 MB test file to the fully functional object storage cluster, consisting of two object storage nodes. \\

\textbf{Technical Background}\footnote{Detailed information on OpenStack Swift is given in \cite{swift} or on \url{https://swiftstack.com/openstack-swift/}}\\
OpenStack Swift stores the copies of the same data on different nodes to prevent the loss of data in case of failure and to be able to offer massive scalability. As opposed to, e.g., a simple file system, Swift is an eventually consistent storage system. Objects in Swift are stored in containers, which contains meta data about the objects. Containers themselves are contained in an account, which is the storage area. The processes that run on a storage node thus are composed of account, container, object and proxy server processes. The proxy server is responsible for the verification and handling of API requests, as well as for deciding which node to send the request to. This includes choosing a substitute storage node, should the primary one be unavailable. To ensure consistency of the data across the nodes, a Swift auditor service continually checks for malformed data on the disks and moves corrupted objects into a specified quarantine. Replicator services check the hashes of the copies of objects on other nodes, and send out their respective copy of the object should it be inconsistent.\\

Regarding the architecture of the storage cluster, it can be divided in nodes, regions and zones. Our system runs two storage nodes with one region and zone, as described in the OpenStack documentation we base our system on. Regions represent geographically separated parts of the cluster, and zones separate physical hardware, i.e. different racks. Our storage nodes contain two storage disks each. Swift partitions storage disks into partitions (which in OpenStack indicate rather a directory than a conventional storage partition), the maximum number of which can be configured - in our case $2^{10} = 1024$. These partitions are replicated as uniquely across the cluster as possible a specified number of times. In our system, three replicas are created. The replicas are mapped to the partitions in a hashing ring. \\

\textbf{Experiment}\\
The experiment contains multiple steps. Aim of the experiment is to verify the availability of a test file download even after the crash of a storage node. Due to the fact that each node has two storage disks, and three replicas of each object are configured, the test file is replicated on one node twice, and on the other once. After the upload of the 100 MB test file, it is downloaded to verify the functionality. Next, a crash of one of the object nodes is simulated. Again, the download is initiated before the node is restarted. This procedure is then done for the simulated crash of the other storage node. In a last step, the crash of both nodes is simulated.\\

\textbf{Results and Conclusion}\\
The results of this experiment show that, for a correctly setup and configured OpenStack system with two storage nodes, a storage node crash will not impair the functionality for the user. The test file object is replicated correctly amongst the nodes, i.e., one node contains two replicas (one on each hard drive), and the other contains one replica. This is due to the fact that Swift tries storing replicas in the most unique way possible amongst the cluster. Only when both storage nodes fail will the download of the test file be impossible. This is an obvious trivial test case that was added to the experiment for completeness.\\

The number of replicas is crucial for Swift to be able to cope with a storage node crash. Three replicas is a configuration that is found often when applying redundancy to high availability systems. However, two replicas would also suffice for surviving a storage node crash in our setup, as Swift distributes the replicas in the most unique way possible, which would assign both nodes one copy of the test file. 





